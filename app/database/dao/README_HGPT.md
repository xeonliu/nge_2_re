# HGPT æ•°æ®åº“è®¾è®¡è¯´æ˜

## è®¾è®¡ç†å¿µ

å‚è€ƒ `EVSDao` çš„è®¾è®¡æ¨¡å¼ï¼ŒHGPT é‡‡ç”¨**å»é‡å­˜å‚¨ + å¯ä¿®æ”¹**æ–¹æ¡ˆï¼š
- ä½¿ç”¨ MD5 hash ä½œä¸ºå”¯ä¸€é”®
- ç›¸åŒçš„å›¾åƒåªå­˜å‚¨ä¸€æ¬¡
- å¤šä¸ª `HgarFile` å¯ä»¥å¼•ç”¨åŒä¸€ä¸ª `Hgpt`
- **æ”¯æŒå›¾åƒä¿®æ”¹**ï¼šPNG å¯ä»¥è¢«ç¼–è¾‘å’Œæ›¿æ¢

## æ ¸å¿ƒæ”¹è¿›ï¼šæ”¯æŒå›¾åƒä¿®æ”¹

### ä¸ºä»€ä¹ˆéœ€è¦æ”¯æŒä¿®æ”¹ï¼Ÿ

åœ¨æ¸¸æˆæœ¬åœ°åŒ–/ç¿»è¯‘è¿‡ç¨‹ä¸­ï¼Œç»å¸¸éœ€è¦ï¼š
- ğŸ“ ç¿»è¯‘å›¾åƒä¸­çš„æ–‡å­—ï¼ˆèœå•ã€æ ‡é¢˜ç­‰ï¼‰
- ğŸ¨ æ›¿æ¢å›¾åƒèµ„æº
- âœï¸ ä¿®æ­£å›¾åƒå†…å®¹

### æ•°æ®å­˜å‚¨ç­–ç•¥

```python
class Hgpt:
    key: str              # MD5 hashï¼ˆå»é‡é”®ï¼‰
    content: bytes        # åŸå§‹ HGPT æ•°æ®ï¼ˆå¤‡ä»½ï¼‰
    png_image: bytes      # PNG æ ¼å¼ï¼ˆå¯ç¼–è¾‘ç‰ˆæœ¬ï¼‰â­
    # ... å…ƒæ•°æ®ï¼ˆå°ºå¯¸ã€æ ¼å¼ç­‰ï¼‰
```

**å…³é”®è®¾è®¡**ï¼š
- `content`: ä¿å­˜åŸå§‹æ•°æ®ä½œä¸ºå¤‡ä»½
- `png_image`: å¯ä»¥è¢«ä¿®æ”¹å’Œæ›¿æ¢ â­
- é‡å»ºæ—¶ä½¿ç”¨ `png_image` è€Œé `content`

## æŠ€æœ¯ä¼˜åŒ–

### æµå¼è¯»å–ï¼ˆæ— ä¸´æ—¶æ–‡ä»¶ï¼‰
`HgptReader` æ”¯æŒä»æ–‡ä»¶æµï¼ˆ`BytesIO`ï¼‰ç›´æ¥è¯»å–æ•°æ®ï¼Œé¿å…åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼š
```python
import io
from app.parser.tools import hgp

# ç›´æ¥ä»å†…å­˜è¯»å–
hgpt_data = b'...'  # è§£å‹åçš„ HGPT æ•°æ®
stream = io.BytesIO(hgpt_data)
reader = hgp.HgptReader(stream)
hgpt_image = reader.read()
```

è¿™æ ·åšçš„å¥½å¤„ï¼š
- âœ… **æ€§èƒ½æå‡**ï¼šé¿å…ç£ç›˜ I/O
- âœ… **å†…å­˜é«˜æ•ˆ**ï¼šæ— éœ€å†™å…¥ä¸´æ—¶æ–‡ä»¶
- âœ… **çº¿ç¨‹å®‰å…¨**ï¼šæ— éœ€ç®¡ç†ä¸´æ—¶æ–‡ä»¶æ¸…ç†
- âœ… **å‘ä¸‹å…¼å®¹**ï¼šä»æ”¯æŒæ–‡ä»¶è·¯å¾„æ–¹å¼

## æ•°æ®åº“ç»“æ„

```
Hgar (å‹ç¼©åŒ…)
  â””â”€â”€ HgarFile (æ–‡ä»¶æ¡ç›®)
        â”œâ”€â”€ Hgpt (å›¾åƒæ•°æ®ï¼Œå»é‡)
        â”œâ”€â”€ EVSEntry (è„šæœ¬æ¡ç›®)
        â””â”€â”€ Raw (å…¶ä»–æ–‡ä»¶)
```

### æ ¸å¿ƒè¡¨

#### `hgpts` - HGPT å›¾åƒæ•°æ®è¡¨
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | Integer | ä¸»é”® |
| `key` | String | MD5 hashï¼ˆå”¯ä¸€ï¼Œç”¨äºå»é‡ï¼‰ |
| `content` | LargeBinary | åŸå§‹ HGPT æ•°æ®ï¼ˆç”¨äºé‡å»ºï¼‰ |
| `png_image` | LargeBinary | PNG å›¾åƒï¼ˆç”¨äºé¢„è§ˆ/ç¿»è¯‘ï¼‰ |
| `width` / `height` | Integer | å›¾åƒå°ºå¯¸ |
| `pp_format` | Integer | åƒç´ æ ¼å¼ (0x13/0x14/0x8800) |
| `palette_size` | Integer | è°ƒè‰²æ¿å¤§å°ï¼ˆRGBA ä¸º NULLï¼‰ |
| `has_extended_header` | Boolean | æ‰©å±•å¤´æ ‡å¿— |
| `division_name` | String | åˆ†åŒºåç§° |
| `divisions` | JSON | åˆ†åŒºä¿¡æ¯ |

#### `hgar_files` - æ–‡ä»¶æ¡ç›®è¡¨
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | Integer | ä¸»é”® |
| `short_name` / `long_name` | String | æ–‡ä»¶å |
| `hgar_id` | Integer | æ‰€å±å‹ç¼©åŒ… |
| `hgpt_key` | String | å¼•ç”¨çš„ HGPTï¼ˆå¯ä¸ºç©ºï¼‰ |
| `file_size` | Integer | åŸå§‹å¤§å° |
| `compressed_size` | Integer | å‹ç¼©å¤§å° |

## ä½¿ç”¨ç¤ºä¾‹

### 1. ä¿å­˜ HGAR åŒ…ï¼ˆè‡ªåŠ¨å¤„ç† HGPTï¼‰

```python
from app.database.dao.hgar_file import HGARFileDao
from app.parser.tools import HGArchiveFile

# è§£æ HGAR å¾—åˆ°æ–‡ä»¶åˆ—è¡¨
hgar_files = [
    HGArchiveFile(short_name=b'scene01.hpt', content=hpt_data1, ...),
    HGArchiveFile(short_name=b'scene02.hpt', content=hpt_data2, ...),
    HGArchiveFile(short_name=b'scene03.hpt', content=hpt_data1, ...),  # é‡å¤å›¾åƒï¼
    HGArchiveFile(short_name=b'script.evs', content=evs_data, ...),
]

# ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
HGARFileDao.save(hgar_id=1, hgar_files=hgar_files)

# è¾“å‡ºï¼š
#   [HPT] scene01.hpt
#   [HGPT] Saved: abc12345... (800x600)
#   [HPT] scene02.hpt
#   [HGPT] Saved: def67890... (1024x768)
#   [HPT] scene03.hpt
#   [HGPT] Duplicate found: abc12345... (skipping)  # è‡ªåŠ¨å»é‡ï¼
#   [EVS] script.evs
```

### 2. é‡å»º HGAR åŒ…

```python
from app.database.dao.hgar_file import HGARFileDao

# ä»æ•°æ®åº“é‡å»ºæ–‡ä»¶åˆ—è¡¨
hgar_files = HGARFileDao.form(hgar_id=1)

# è¾“å‡ºï¼š
#   Rebuilding: scene01.hpt
#   Rebuilding: scene02.hpt
#   Rebuilding: scene03.hpt  # ä½¿ç”¨ç›¸åŒçš„ hgpt_keyï¼Œè‡ªåŠ¨å¤ç”¨æ•°æ®
#   Rebuilding: script.evs

# æ‰“åŒ…æˆ HGAR
for file in hgar_files:
    write_to_archive(file.short_name, file.content)
```

### 3. å®Œæ•´å·¥ä½œæµï¼ˆè§£æ â†’ ç¿»è¯‘ â†’ é‡å»ºï¼‰

```python
# Step 1: è§£æå¹¶ä¿å­˜
hgar_wrapper = HGARWrapper()
hgar_wrapper.open('/path/to/archive.har')
HGARFileDao.save(hgar_id=1, hgar_files=hgar_wrapper.files)

# Step 2: ç¿»è¯‘å·¥ä½œ
# - EVS æ–‡æœ¬é€šè¿‡ sentences/translations è¡¨å¤„ç†
# - HGPT å›¾åƒå¯ä»¥å¯¼å‡º PNG è¿›è¡Œç¿»è¯‘

# Step 3: é‡å»º HGAR
rebuilt_files = HGARFileDao.form(hgar_id=1)
hgar_wrapper.files = rebuilt_files
hgar_wrapper.save('/path/to/translated_archive.har')
```

### 4. æŸ¥è¯¢å’Œç»Ÿè®¡

```python
from app.database.entity.hgpt import Hgpt
from app.database.entity.hgar_file import HgarFile
from sqlalchemy import func

# æŸ¥æ‰¾é‡å¤ä½¿ç”¨çš„å›¾åƒ
duplicates = db.query(
    Hgpt.key,
    Hgpt.width,
    Hgpt.height,
    func.count(HgarFile.id).label('usage_count')
).join(HgarFile).group_by(Hgpt.key).having(
    func.count(HgarFile.id) > 1
).all()

for key, w, h, count in duplicates:
    print(f"å›¾åƒ {key[:8]}... ({w}x{h}) è¢«ä½¿ç”¨äº† {count} æ¬¡")

# è¾“å‡ºç¤ºä¾‹ï¼š
# å›¾åƒ abc12345... (800x600) è¢«ä½¿ç”¨äº† 15 æ¬¡
# å›¾åƒ def67890... (1024x768) è¢«ä½¿ç”¨äº† 8 æ¬¡
```

## å»é‡æ•ˆæœ

å‡è®¾æœ‰ 1000 ä¸ª HGAR æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å« 5000 ä¸ªå›¾åƒå¼•ç”¨ï¼Œä½†å®é™…åªæœ‰ 2000 ä¸ªä¸åŒçš„å›¾åƒï¼š

- **æ— å»é‡**: å­˜å‚¨ 5000 ä¸ªå›¾åƒå‰¯æœ¬
- **æœ‰å»é‡**: å­˜å‚¨ 2000 ä¸ªå”¯ä¸€å›¾åƒ + 5000 ä¸ªå¼•ç”¨è®°å½•

èŠ‚çœç©ºé—´çº¦ 60%ï¼

## ä¸ EVS çš„å¯¹æ¯”

| ç‰¹æ€§ | EVS | HGPT |
|------|-----|------|
| å»é‡é”® | `Sentence.key` (MD5) | `Hgpt.key` (MD5) |
| å†…å®¹å­˜å‚¨ | `Sentence.content` (æ–‡æœ¬) | `Hgpt.content` (äºŒè¿›åˆ¶) |
| é¢å¤–æ•°æ® | `Translation` (ç¿»è¯‘) | `png_image` (é¢„è§ˆ) |
| å¼•ç”¨æ–¹å¼ | `EVSEntry.sentence_key` | `HgarFile.hgpt_key` |
| DAO æ–¹æ³• | `save()` / `form_evs_wrapper()` | `save()` / `get_hgpt_data()` |

ä¸¤è€…éƒ½éµå¾ªç›¸åŒçš„è®¾è®¡æ¨¡å¼ï¼š**å†…å®¹å»é‡ + å¼•ç”¨å…³è”**ã€‚
