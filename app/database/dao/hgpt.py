"""
Persist HGPT Image
参考 EVSDao 的设计模式实现 HGPT 的去重存储和重建
"""

import hashlib
import io
import os
import shutil
from pathlib import Path
from typing import Dict, List, Tuple

from ..db import get_db
from app.parser.tools import hgp, png

from ..entity.hgpt import Hgpt


class HgptDao:
    @staticmethod
    def save(hgpt_data: bytes) -> str:
        """
        保存 HGPT 数据到数据库（去重）
        
        Args:
            hgpt_data: 原始解压后的 HGPT 二进制数据
            
        Returns:
            str: HGPT 的 hash key
        """
        # 1. 计算 hash（用于去重）
        hash_object = hashlib.md5(hgpt_data)
        hashed_key = hash_object.hexdigest()
        
        with next(get_db()) as db:
            # 2. 检查是否已存在
            existing = db.query(Hgpt).filter(Hgpt.key == hashed_key).first()
            if existing:
                print(f"  [HGPT] Duplicate found: {hashed_key[:8]}... (skipping)")
                return hashed_key
            
            # 3. 解析 HGPT 文件（使用 BytesIO，无需临时文件）
            try:
                # 使用 BytesIO 将字节数据转换为文件流
                file_stream = io.BytesIO(hgpt_data)
                reader = hgp.HgptReader(file_stream)
                hgpt_image = reader.read()
                
                # 4. 导出为 PNG（用于预览和翻译）
                png_data = HgptDao._export_to_png(hgpt_image)
                
                # 5. 提取格式信息
                pp_format = HgptDao._get_pp_format(hgpt_image)
                palette_size = len(hgpt_image.palette) if (hgpt_image.palette and hasattr(hgpt_image.palette, '__len__')) else None
                
                # 6. 创建数据库记录
                hgpt_record = Hgpt(
                    key=hashed_key,
                    content=hgpt_data,
                    png_image=png_data,
                    has_extended_header=hgpt_image.header.has_extended_header,
                    unknown_two=hgpt_image.header.unknown_two,
                    unknown_three=hgpt_image.header.unknown_three,
                    width=hgpt_image.display_info.width,
                    height=hgpt_image.display_info.height,
                    pp_format=pp_format,
                    palette_size=palette_size,
                    division_name=hgpt_image.division_info.name if hgpt_image.division_info else None,
                    divisions=hgpt_image.division_info.divisions if hgpt_image.division_info else None,
                )
                
                db.add(hgpt_record)
                db.commit()
                
                print(f"  [HGPT] Saved: {hashed_key[:8]}... ({hgpt_image.display_info.width}x{hgpt_image.display_info.height})")
                return hashed_key
                
            except Exception as e:
                print(f"  [HGPT] Parse error: {e}")
                # 如果解析失败，仍然保存原始数据
                hgpt_record = Hgpt(
                    key=hashed_key,
                    content=hgpt_data,
                    png_image=None,
                    has_extended_header=False,
                    unknown_two=0,
                    unknown_three=0x0013,
                    width=0,
                    height=0,
                    pp_format=None,
                    palette_size=None,
                )
                db.add(hgpt_record)
                db.commit()
                return hashed_key
    
    @staticmethod
    def get_hgpt_data(hgpt_key: str) -> bytes:
        """
        重建 HGPT 数据（从数据库中的结构化数据重新生成）
        优先使用翻译后的 PNG，如果没有则使用原始 PNG
        
        Args:
            hgpt_key: HGPT 的 hash key
            
        Returns:
            bytes: 重新生成的 HGPT 二进制数据
        """
        with next(get_db()) as db:
            hgpt = db.query(Hgpt).filter(Hgpt.key == hgpt_key).first()
            if not hgpt:
                raise ValueError(f"HGPT not found: {hgpt_key}")
            
            # 优先使用翻译后的 PNG，否则使用原始 PNG
            if hgpt.png_translated or hgpt.png_image:
                return HgptDao._rebuild_from_png(hgpt)
            else:
                # 回退：如果没有 PNG，使用原始数据
                return hgpt.content
    
    @staticmethod
    def get_png_image(hgpt_key: str) -> bytes:
        """
        获取 PNG 图像数据（用于预览）
        
        Args:
            hgpt_key: HGPT 的 hash key
            
        Returns:
            bytes: PNG 图像数据
        """
        with next(get_db()) as db:
            hgpt = db.query(Hgpt).filter(Hgpt.key == hgpt_key).first()
            if not hgpt:
                raise ValueError(f"HGPT not found: {hgpt_key}")
            return hgpt.png_image
    
    @staticmethod
    def _export_to_png(hgpt_image: hgp.HgptImage) -> bytes:
        """
        将 HgptImage 对象导出为 PNG 字节数据
        
        Args:
            hgpt_image: 解析后的 HGPT 图像对象
            
        Returns:
            bytes: PNG 图像数据
        """
        output = io.BytesIO()
        
        if hgpt_image.palette:
            # 调色板模式
            # PNG Writer 需要 (r,g,b,a) 元组列表
            palette_colors = hgpt_image.palette.colors
            
            w = png.Writer(
                width=hgpt_image.display_info.width,
                height=hgpt_image.display_info.height,
                palette=palette_colors,
                bitdepth=8
            )
            rows = [
                hgpt_image.content[i:i + hgpt_image.display_info.width]
                for i in range(0, len(hgpt_image.content), hgpt_image.display_info.width)
            ]
        else:
            # RGBA 模式
            w = png.Writer(
                width=hgpt_image.display_info.width,
                height=hgpt_image.display_info.height,
                greyscale=False,
                alpha=True
            )
            rows = [
                hgpt_image.content[i:i + hgpt_image.display_info.width]
                for i in range(0, len(hgpt_image.content), hgpt_image.display_info.width)
            ]
        
        w.write(output, rows)
        return output.getvalue()
    
    @staticmethod
    def _get_pp_format(hgpt_image: hgp.HgptImage) -> int:
        """
        推断 PP format（从 palette 或内容推断）
        
        Args:
            hgpt_image: 解析后的 HGPT 图像对象
            
        Returns:
            int: PP format 值
        """
        if hgpt_image.palette:
            palette_len = len(hgpt_image.palette)
            if palette_len <= 16:
                return 0x14  # 4-bit paletted
            else:
                return 0x13  # 8-bit paletted
        else:
            return 0x8800  # 32-bit RGBA
    
    @staticmethod
    def _rebuild_from_png(hgpt: Hgpt) -> bytes:
        """
        从数据库中的 PNG 和元数据重建 HGPT 文件
        优先使用翻译后的 PNG，如果没有则使用原始 PNG
        
        Args:
            hgpt: Hgpt 数据库记录
            
        Returns:
            bytes: 重新生成的 HGPT 二进制数据
        """
        # 1. 读取 PNG 图像（优先使用翻译版本）
        png_data = hgpt.png_translated if hgpt.png_translated else hgpt.png_image
        pr = png.Reader(bytes=png_data)
        width, height, rows, info = pr.read()
        
        # 2. 重建 HgptHeader
        header = hgp.HgptHeader()
        header.has_extended_header = hgpt.has_extended_header
        header.unknown_two = hgpt.unknown_two
        header.unknown_three = hgpt.unknown_three
        
        # 3. 重建 DisplayInfo
        display_info = hgp.DisplayInfo(width=hgpt.width, height=hgpt.height)
        
        # 4. 重建 DivisionInfo（如果有）
        division_info = None
        if hgpt.division_name or hgpt.divisions:
            division_info = hgp.DivisionInfo(
                name=hgpt.division_name or '',
                divisions=hgpt.divisions or []
            )
        
        # 5. 重建图像内容和调色板
        palette = None
        content = []
        
        if 'palette' in info:
            # 调色板模式
            colors = [(c[0], c[1], c[2], c[3] if len(c) > 3 else 255) for c in info['palette']]
            
            # 扩展调色板到标准大小
            if 0 < len(colors) < 16:
                colors.extend([(0, 0, 0, 255)] * (16 - len(colors)))
            elif 16 < len(colors) < 256:
                colors.extend([(0, 0, 0, 255)] * (256 - len(colors)))
            
            palette = hgp.Palette(colors)
            content = [pixel for row in rows for pixel in row]
        else:
            # RGBA 模式
            pixel_depth = 4 if info['alpha'] else 3
            rows_list = list(rows)
            for row in rows_list:
                for i in range(0, len(row), pixel_depth):
                    r, g, b = row[i], row[i+1], row[i+2]
                    a = row[i+3] if info['alpha'] else 255
                    content.append((r, g, b, a))
        
        # 6. 构建 HgptImage 对象
        hgpt_image = hgp.HgptImage(
            header=header,
            display_info=display_info,
            content=content,
            palette=palette,
            division_info=division_info
        )
        
        # 7. 写入到内存流（使用 BytesIO，无需临时文件）
        output = io.BytesIO()
        writer = hgp.HgptWriter(hgpt_image)
        writer.write(output)
        return output.getvalue()
    
    @staticmethod
    def import_translated_png(hgpt_key: str, translated_png_data: bytes):
        """
        导入翻译后的 PNG 图像（存储到 png_translated 字段）
        
        Args:
            hgpt_key: HGPT 的 hash key
            translated_png_data: 翻译后的 PNG 图像数据
        """
        with next(get_db()) as db:
            hgpt = db.query(Hgpt).filter(Hgpt.key == hgpt_key).first()
            if not hgpt:
                raise ValueError(f"HGPT not found: {hgpt_key}")
            
            # 验证 PNG 尺寸（必须与原始图像一致）
            pr = png.Reader(bytes=translated_png_data)
            width, height, _, _ = pr.read()
            if width != hgpt.width or height != hgpt.height:
                raise ValueError(
                    f"Size mismatch: expected {hgpt.width}x{hgpt.height}, "
                    f"got {width}x{height}"
                )
            
            # 存储翻译版本
            hgpt.png_translated = translated_png_data
            db.commit()
            print(f"  [HGPT] Imported translation for {hgpt_key[:8]}... ({width}x{height})")
    
    @staticmethod
    def export_all_images(output_dir: str) -> Dict[str, List[Tuple[str, str]]]:
        """
        导出所有 HGPT 图像到指定目录
        按照 HAR 文件组织，文件名包含短名称和 hash
        只创建包含图像的目录
        
        Args:
            output_dir: 输出目录路径
            
        Returns:
            Dict[har_name, List[(filename, hgpt_key)]]: 导出的文件映射
        """
        from ..entity.hgar import Hgar
        from ..entity.hgar_file import HgarFile
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        exported_images = {}
        seen_keys = set()  # 用于去重
        har_images_temp = {}  # 临时存储每个 HAR 的图像
        
        with next(get_db()) as db:
            # 遍历所有 HGAR 文件
            hgar_list = db.query(Hgar).all()
            
            for hgar in hgar_list:
                har_name = hgar.name.replace('.har', '')
                har_images_temp[har_name] = []
                
                # 遍历该 HAR 中的所有文件
                for hgar_file in hgar.files:
                    if not hgar_file.hgpt_key:
                        continue
                    
                    # 去重：同一图像只导出一次
                    if hgar_file.hgpt_key in seen_keys:
                        continue
                    seen_keys.add(hgar_file.hgpt_key)
                    
                    # 获取 HGPT 记录
                    hgpt = db.query(Hgpt).filter(Hgpt.key == hgar_file.hgpt_key).first()
                    if not hgpt or not hgpt.png_image:
                        continue
                    
                    # 生成文件名：短名称_hash[:8].png
                    short_name = hgar_file.short_name or f"id{hgar_file.identifier}"
                    short_name = short_name.replace('.hpt', '').replace('.zpt', '')
                    filename = f"{short_name}_{hgpt.key[:8]}.png"
                    
                    har_images_temp[har_name].append((filename, hgpt.png_image))
                
                # 只有当该 HAR 有图像时，才创建目录并导出
                if har_images_temp[har_name]:
                    har_dir = output_path / har_name
                    har_dir.mkdir(exist_ok=True)
                    
                    exported_images[har_name] = []
                    for filename, png_data in har_images_temp[har_name]:
                        # 导出原始 PNG（用于翻译）
                        file_path = har_dir / filename
                        with open(file_path, 'wb') as f:
                            f.write(png_data)
                        
                        # 从文件名提取 hash
                        hash_part = filename.split('_')[-1].replace('.png', '')
                        exported_images[har_name].append((filename, hash_part))
                    
                    print(f"  [Export] {har_name}: {len(exported_images[har_name])} images")
            
            # 删除空文件夹
            for item in output_path.iterdir():
                if item.is_dir() and not list(item.iterdir()):
                    shutil.rmtree(item)
                    print(f"  [Export] Removed empty directory: {item.name}")
        
        total_images = sum(len(imgs) for imgs in exported_images.values())
        print(f"\n  [Export] Total: {total_images} unique images exported to {output_dir}")
        return exported_images
    
    @staticmethod
    def import_translated_images(translation_dir: str) -> int:
        """
        从指定目录批量导入翻译后的图像
        根据文件名中的 hash 匹配图像
        
        Args:
            translation_dir: 包含翻译图像的目录
            
        Returns:
            int: 成功导入的图像数量
        """
        translation_path = Path(translation_dir)
        if not translation_path.exists():
            raise ValueError(f"Translation directory not found: {translation_dir}")
        
        imported_count = 0
        
        # 递归查找所有 PNG 文件
        png_files = list(translation_path.rglob("*.png"))
        
        print(f"  [Import] Found {len(png_files)} PNG files in {translation_dir}")
        
        with next(get_db()) as db:
            for png_file in png_files:
                # 从文件名提取 hash（格式：xxx_HASH8.png）
                filename = png_file.stem  # 去掉 .png
                parts = filename.split('_')
                
                if len(parts) < 2:
                    print(f"  [Import] Skip (no hash): {png_file.name}")
                    continue
                
                hash_short = parts[-1]  # 最后一部分是 hash
                
                # 在数据库中查找匹配的 HGPT（hash 前缀匹配）
                hgpt_list = db.query(Hgpt).filter(Hgpt.key.like(f"{hash_short}%")).all()
                
                if not hgpt_list:
                    print(f"  [Import] Skip (not found): {png_file.name} (hash: {hash_short})")
                    continue
                
                if len(hgpt_list) > 1:
                    print(f"  [Import] Warning: Multiple matches for {hash_short}, using first")
                
                hgpt = hgpt_list[0]
                
                # 读取翻译后的 PNG
                with open(png_file, 'rb') as f:
                    translated_png = f.read()
                
                # 验证尺寸
                try:
                    pr = png.Reader(bytes=translated_png)
                    width, height, _, _ = pr.read()
                    
                    if width != hgpt.width or height != hgpt.height:
                        print(f"  [Import] Skip (size mismatch): {png_file.name} "
                              f"(expected {hgpt.width}x{hgpt.height}, got {width}x{height})")
                        continue
                    
                    # 导入翻译版本
                    hgpt.png_translated = translated_png
                    imported_count += 1
                    print(f"  [Import] ✓ {png_file.name} -> {hgpt.key[:8]}...")
                    
                except Exception as e:
                    print(f"  [Import] Error: {png_file.name} - {e}")
                    continue
            
            db.commit()
        
        print(f"\n  [Import] Successfully imported {imported_count} translated images")
        return imported_count
